[
    {
        "id": 0,
        "title": "Diffusion-Based Neural Network Weights Generation",
        "abstract": "Transfer learning has gained significant attention in recent deep learning research due to its ability to accelerate convergence and enhance performance on new tasks. However, its success is often contingent on the similarity between source and target data, and training on numerous datasets can be costly, leading to blind selection of pretrained models with limited insight into their effectiveness. To address these challenges, we introduce D2NWG, a diffusion-based neural network weights generation technique that efficiently produces high-performing weights for transfer learning, conditioned on the target dataset. Our method extends generative hyperrepresentation learning to recast the latent diffusion paradigm for neural network weights generation, learning the weight distributions of models pretrained on various datasets. This allows for automatic generation of weights that generalize well across both seen and unseen tasks, outperforming state-of-the-art meta-learning methods and pretrained models. Moreover, our approach is scalable to large architectures such as large language models (LLMs), overcoming the limitations of current parameter generation techniques that rely on task-specific model collections or access to original training data. By modeling the parameter distribution of LLMs, D2NWG enables task-specific parameter generation without requiring additional fine-tuning or large collections of model variants. Extensive experiments show that our method consistently enhances the performance of diverse base models, regardless of their size or complexity, positioning it as a robust solution for scalable transfer learning.",
        "author": [
            "Bedionita Soro", "Bruno Andreis", "Hayeon Lee", "Wonyong Jeong", "Song Chong", "Frank Hutter", "Sung Ju Hwang"
        ],
        "citation": [
            "Graph HyperNetworks for Neural Architecture Search",
            "Parameter Prediction for Unseen Deep Architectures",
            "Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?"

        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "LLM", "Weight Generation", "Diffusion"
        ],
        "conference": "ICLR",
        "url": "https://arxiv.org/abs/2402.18153"
    },

    {
        "id": 1,
        "title": "Conditional LoRA Parameter Generation",
        "abstract": "Generative models have achieved remarkable success in image, video, and text domains. Inspired by this, researchers have explored utilizing generative models to generate neural network parameters. However, these efforts have been limited by the parameter size and the practicality of generating high-performance parameters. In this paper, we propose COND P-DIFF, a novel approach that demonstrates the feasibility of controllable high-performance parameter generation, particularly for LoRA (Low-Rank Adaptation) weights, during the fine-tuning process. Specifically, we employ an autoencoder to extract efficient latent representations for parameters. We then train a conditional latent diffusion model to synthesize high-performing model parameters from random noise based on specific task conditions. Experimental results in both computer vision and natural language processing domains consistently demonstrate that COND P-DIFF can generate high-performance parameters conditioned on the given task. Moreover, we observe that the parameter distribution generated by COND P-DIFF exhibits differences compared to the distribution obtained through normal optimization methods, indicating a certain level of generalization capability. Our work paves the way for further exploration of condition-driven parameter generation, offering a promising direction for task-specific adaptation of neural networks.",
        "author": [
            "Xiaolong Jin", "Kai Wang", "Dongwen Tang", "Wangbo Zhao", "Yukun Zhou", "Junshu Tang", "Yang You"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Parameter Prediction for Unseen Deep Architectures"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Weight Generation", "LoRA", "Diffusion"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2408.01415"
    },

    {
        "id": 2,
        "title": "Weight Diffusion for Future: Learn to Generalize in Non-Stationary Environments",
        "abstract": "Enabling deep models to generalize in non-stationary environments is vital for real-world machine learning, as data distributions are often found to continually change. Recently, evolving domain generalization (EDG) has emerged to tackle the domain generalization in a time-varying system, where the domain gradually evolves over time in an underlying continuous structure. Nevertheless, it typically assumes multiple source domains simultaneously ready. It still remains an open problem to address EDG in the domain-incremental setting, where source domains are non-static and arrive sequentially to mimic the evolution of training domains. To this end, we propose Weight Diffusion (W-Diff), a novel framework that utilizes the conditional diffusion model in the parameter space to learn the evolving pattern of classifiers during the domain-incremental training process. Specifically, the diffusion model is conditioned on the classifier weights of different historical domain (regarded as a reference point) and the prototypes of current domain, to learn the evolution from the reference point to the classifier weights of current domain (regarded as the anchor point). In addition, a domain-shared feature encoder is learned by enforcing prediction consistency among multiple classifiers, so as to mitigate the overfitting problem and restrict the evolving pattern to be reflected in the classifier as much as possible. During inference, we adopt the ensemble manner based on a great number of target domain-customized classifiers, which are cheaply obtained via the conditional diffusion model, for robust prediction. Comprehensive experiments on both synthetic and real-world datasets show the superior generalization performance of W-Diff on unseen domains in the future.",
        "author": [
            "Mixue Xie", "Shuang Li", "Binhui Xie", "Chi Harold Liu", "Jian Liang", "Zixun Sun", "Ke Feng", "Chengwei Zhu"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Diffusion", "Weight", "Domain Generalization", "Incremental Learning"
        ],
        "conference": "NeurIPS",
        "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/0c1124bd3be769dacf491d92d499c7d8-Abstract-Conference.html"
    },

    {
        "id": 3,
        "title": "Recurrent Diffusion for Large-Scale Parameter Generation",
        "abstract": "Parameter generation has long struggled to match the scale of today large vision and language models, curbing its broader utility. In this paper, we introduce Recurrent Diffusion for Large Scale Parameter Generation (RPG), a novel framework that generates full neural network parameters up to hundreds of millions on a single GPU. Our approach first partitions a networks parameters into non-overlapping tokens, each corresponding to a distinct portion of the model. A recurrent mechanism then learns the inter token relationships, producing prototypes which serve as conditions for a diffusion process that ultimately synthesizes the full parameters. Across a spectrum of architectures and tasks including ResNets, ConvNeXts and ViTs on ImageNet 1K and COCO, and even LoRA based LLMs RPG achieves performance on par with fully trained networks while avoiding excessive memory overhead. Notably, it generalizes beyond its training set to generate valid parameters for previously unseen tasks, highlighting its flexibility in dynamic and open ended scenarios. By overcoming the longstanding memory and scalability barriers, RPG serves as a critical advance in AI generating AI, potentially enabling efficient weight generation at scales previously deemed infeasible.",
        "author": [
            "Kai Wang", "Dongwen Tang", "Wangbo Zhao", "Konstantin Sch√ºrholt", "Zhangyang Wang", "Yang You"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Conditional LoRA Parameter Generation",
            "Unleash Graph Neural Networks from Heavy Tuning"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Diffusion", "Weight Generation"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2501.11587"
    },

    {
        "id": 4,
        "title": "Unleash Graph Neural Networks from Heavy Tuning",
        "abstract": "Graph Neural Networks (GNNs) are deep-learning architectures designed for graph-type data, where understanding relationships among individual observations is crucial. However, achieving promising GNN performance, especially on unseen data, requires comprehensive hyperparameter tuning and meticulous training. Unfortunately, these processes come with high computational costs and significant human effort. Additionally, conventional searching algorithms such as grid search may result in overfitting on validation data, diminishing generalization accuracy. To tackle these challenges, we propose a graph conditional latent diffusion framework (GNN-Diff) to generate high-performing GNNs directly by learning from checkpoints saved during a light-tuning coarse search. Our method: (1) unleashes GNN training from heavy tuning and complex search space design; (2) produces GNN parameters that outperform those obtained through comprehensive grid search; and (3) establishes higher-quality generation for GNNs compared to diffusion frameworks designed for general neural networks.",
        "author": [
            "Lequan Lin", "Dai Shi", "Andi Han", "Zhiyong Wang", "Junbin Gao"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Graph HyperNetworks for Neural Architecture Search"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "GNN"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2405.12521"
    },

    {
        "id": 5,
        "title": "ReALLM: A general framework for LLM compression and fine-tuning",
        "abstract": "We introduce ReALLM, a novel approach for compression and memory-efficient adaptation of pre-trained language models that encompasses most of the post-training quantization and fine-tuning methods for a budget of <4 bits. Pre-trained matrices are decomposed into a high-precision low-rank component and a vector-quantized latent representation (using an autoencoder). During the fine-tuning step, only the low-rank components are updated. Our results show that pre-trained matrices exhibit different patterns. ReALLM adapts the shape of the encoder (small/large embedding, high/low bit VQ, etc.) to each matrix. ReALLM proposes to represent each matrix with a small embedding on b bits and a neural decoder model Óà∞œï with its weights on bœï bits. The decompression of a matrix requires only one embedding and a single forward pass with the decoder. Our weight-only quantization algorithm yields the best results on language generation tasks (C4 and WikiText-2) for a budget of 3 bits without any training. With a budget of 2 bits, ReALLM achieves state-of-the art performance after fine-tuning on a small calibration dataset.",
        "author": [
            "Louis Leconte", "Lisa Bedin", "Van Minh Nguyen", "Eric Moulines"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "LLM", "Compression", "Quantization"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2405.13155"
    },

    {
        "id": 6,
        "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion",
        "abstract": "Parameter generation has emerged as a novel paradigm for neural network development, offering an alternative to traditional neural network training by synthesizing high-quality model weights directly. In the context of Low-Rank Adaptation (LoRA) for evolving (i.e., constantly updated) large language models (LLMs), this approach promises efficient adaptation without costly retraining. However, existing methods face critical limitations in simultaneously achieving scalability and controllability. In this paper, we introduce ùôæùöÅùô∞ùôª, a novel conditional recurrent diffusion framework that addresses these challenges. ùôæùöÅùô∞ùôª incorporates a novel conditioning mechanism that integrates model architecture and textual task specifications, enabling the generation of task-specific LoRA parameters that can seamlessly transfer across evolving foundation models. Our approach successfully scales to billions-of-parameter LLMs and maintains controllability. Through extensive experiments across seven language tasks, four vision tasks, and three multimodal tasks using five pre-trained LLMs, we demonstrate that ùôæùöÅùô∞ùôª generates high-quality LoRA parameters that achieve comparable or superior performance to vanilla trained counterparts.",
        "author": [
            "Rana Muhammad Shahroz Khan", "Dongwen Tang", "Pingzhi Li", "Kai Wang", "Tianlong Chen"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Unleash Graph Neural Networks from Heavy Tuning"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Diffusion", "Weight"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2503.24354"
    },

    {
        "id": 7,
        "title": "Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning",
        "abstract": "Graph Neural Networks (GNNs) are proficient in graph representation learning and achieve promising performance on versatile tasks such as node classification and link prediction. Usually, a comprehensive hyperparameter tuning is essential for fully unlocking GNN's top performance, especially for complicated tasks such as node classification on large graphs and long-range graphs. This is usually associated with high computational and time costs and careful design of appropriate search spaces. This work introduces a graph-conditioned latent diffusion framework (GNN-Diff) to generate high-performing GNNs based on the model checkpoints of sub-optimal hyperparameters selected by a light-tuning coarse search. We validate our method through 166 experiments across four graph tasks: node classification on small, large, and long-range graphs, as well as link prediction. Our experiments involve 10 classic and state-of-the-art target models and 20 publicly available datasets. The results consistently demonstrate that GNN-Diff: (1) boosts the performance of GNNs with efficient hyperparameter tuning; and (2) presents high stability and generalizability on unseen data across multiple generation runs. The code is available at this https URL.",
        "author": [
            "Lequan Lin", "Dai Shi", "Andi Han", "Zhiyong Wang", "Junbin Gao"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Graph HyperNetworks for Neural Architecture Search"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "GNN", "Diffusion"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2410.05697"
    },

    {
        "id": 8,
        "title": "Set-based Neural Network Encoding Without Weight Tying",
        "abstract": "We propose a neural network weight encoding method for network property prediction that utilizes set-to-set and set-to-vector functionsto efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a model zoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks. To respect symmetries inherent in network weight space, we utilize Logit Invariance to learn the required minimal invariance properties. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network property prediction: cross-dataset and cross-architecture. In cross-dataset property prediction, we evaluate how well property predictors generalize across model zoos trained on different datasets but of the same architecture. In cross-architecture property prediction, we evaluate how well property predictors transfer to model zoos of different architecture not seen during training. We show that SNE outperforms the relevant baselines on standard benchmarks.",
        "author": [
            "Bruno Andreis", "Soro Bedionita", "Philip H.S. Torr", "Sung Ju Hwang"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Weight", "Encoding", "Prediction", "Set"
        ],
        "conference": "NeurIPS",
        "url": "https://arxiv.org/abs/2305.16625"
    },

    {
        "id": 9,
        "title": "Flow to Learn: Flow Matching on Neural Network Parameters",
        "abstract": "Foundational language models show a remarkable ability to learn new concepts during inference via context data. However, similar work for images lag behind. To address this challenge, we introduce FLoWN, a flow matching model that learns to generate neural network parameters for different tasks. Our approach models the flow on latent space, while conditioning the process on context data. Experiments verify that FLoWN attains various desiderata for a meta-learning model. In addition, it matches or exceeds baselines on in-distribution tasks, provides better initializations for classifier training, and is performant on out-of-distribution few-shot tasks while having a fine-tuning mechanism to improve performance.",
        "author": [
            "Daniel Saragih", "Deyu Cao", "Tejas Balaji", "Ashwin Santhosh"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Encoding", "Weight", "Flow"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2503.19371"
    },

    {
        "id": 10,
        "title": "Structure Is Not Enough: Leveraging Behavior for Neural Network Weight Reconstruction",
        "abstract": "The weights of neural networks (NNs) have recently gained prominence as a new data modality in machine learning, with applications ranging from accuracy and hyperparameter prediction to representation learning or weight generation. One approach to leverage NN weights involves training autoencoders (AEs), using contrastive and reconstruction losses. This allows such models to be applied to a wide variety of downstream tasks, and they demonstrate strong predictive performance and low reconstruction error. However, despite the low reconstruction error, these AEs reconstruct NN models with deteriorated performance compared to the original ones, limiting their usability with regard to model weight generation. In this paper, we identify a limitation of weight-space AEs, specifically highlighting that a structural loss, that uses the Euclidean distance between original and reconstructed weights, fails to capture some features critical for reconstructing high-performing models. We analyze the addition of a behavioral loss for training AEs in weight space, where we compare the output of the reconstructed model with that of the original one, given some common input. We show a strong synergy between structural and behavioral signals, leading to increased performance in all downstream tasks evaluated, in particular NN weights reconstruction and generation.",
        "author": [
            "L√©o Meynent", "Ivan Melev", "Konstantin Sch√ºrholt", "G√∂ran Kauermann", "Damian Borth"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Conditional LoRA Parameter Generation",
            "Graph HyperNetworks for Neural Architecture Search",
            "Parameter Prediction for Unseen Deep Architectures",
            "Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Weight Sapce", "Representation Learning"
        ],
        "conference": "ICLR Workshop",
        "url": "https://www.arxiv.org/abs/2503.17138"
    },

    {
        "id": 11,
        "title": "Enhancing Accuracy and Parameter-Efficiency of Neural Representations for Network Parameterization",
        "abstract": "In this work, we investigate the fundamental trade-off regarding accuracy and parameter efficiency in the parameterization of neural network weights using predictor networks. We present a surprising finding that, when recovering the original model accuracy is the sole objective, it can be achieved effectively through the weight reconstruction objective alone. Additionally, we explore the underlying factors for improving weight reconstruction under parameter-efficiency constraints, and propose a novel training scheme that decouples the reconstruction objective from auxiliary objectives such as knowledge distillation that leads to significant improvements compared to state-of-the-art approaches. Finally, these results pave way for more practical scenarios, where one needs to achieve improvements on both model accuracy and predictor network parameter-efficiency simultaneously.",
        "author": [
            "Hongjun Choi", "Jayaraman J. Thiagarajan", "Ruben Glatt", "Shusen Liu"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Weight Sapce", "Implicit Neural Representation", "Knowlwdge Distillation"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2407.00356"
    },

    {
        "id": 12,
        "title": "Puppet-CNN: Input-Adaptive Convolutional Neural Networks with Model Compression using Ordinary Differential Equation",
        "abstract": "Convolutional Neural Network (CNN) has been applied to more and more scenarios due to its excellent performance in many machine learning tasks, especially with deep and complex structures. However, as the network goes deeper, more parameters need to be stored and optimized. Besides, almost all common CNN models adopt 'train-and-use' strategy where the structure is pre-defined and the kernel parameters are fixed after the training with the same structure and set of parameters used for all data without considering the content complexity. In this paper, we propose a new CNN framework, named as Puppet-CNN, which contains two modules: a puppet module and a puppeteer module. The puppet module is a CNN model used to actually process the input data just like other works, but its depth and kernels are generated by the puppeteer module (realized with Ordinary Differential Equation (ODE)) based on the input complexity each time. By recurrently generating kernel parameters in the puppet module, we can take advantage of the dependence among kernels of different convolutional layers to significantly reduce the size of CNN model by only storing and training the parameters of the much smaller puppeteer ODE module. Through experiments on several datasets, our method has proven to be superior than the traditional CNNs on both performance and efficiency. The model size can be reduced more than 10 times.",
        "author": [
            "Yucheng Xing", "Xin Wang"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "CNN", "Compression"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2411.12876"
    },

    {
        "id": 13,
        "title": "Structure and Behavior in Weight Space Representation Learning",
        "abstract": "The weights of neural networks (NNs) have recently gained prominence as a new data modality in machine learning, with applications ranging from accuracy and hyperparameter prediction to representation learning or weight generation. One approach to leverage NN weights involves training autoencoders (AEs) with contrastive and reconstruction losses. Indeed, such models can be applied to a wide variety of downstream tasks, and they demonstrate strong predictive performance and low reconstruction error. However, despite the low reconstruction error, these AEs reconconstruct NN models that fail to match the performance of the original ones. In this paper, we identify a limitation of weight-space AEs, specifically highlighting that structural weight reconstruction alone fails to capture some features critical for reconstructing high-performing models. To address this issue, we propose a behavioral loss for training AEs in weight space. This behavioral loss focuses on the features essential for reconstructing performant models, which are not adequately captured by structural reconstruction. We evaluate the capabilities of AE trained using this novel loss on three different model zoos: we demonstrate that when combining structural and behavioral losses, we can reconstruct and generate models that match the performance of the original models. With our exploration of representation learning in deep weight spaces, we show that a strong synergy exists between structural and behavioral features, and that combining them results in increased performance across all evaluated downstream tasks.",
        "author": [
            "L√©o Meynent", "Ivan Melev", "Konstantin Sch√ºrholt", "Goeran Kauermann", "Damian Borth"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Conditional LoRA Parameter Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Weight Space", "Representation Learning"
        ],
        "conference": "",
        "url": "https://openreview.net/forum?id=GOwNImvCWf"
    },

    {
        "id": 14,
        "title": "Model Zoos for Benchmarking Phase Transitions in Neural Networks",
        "abstract": "Understanding the complex dynamics of neural network training remains a central challenge in deep learning research. Work rooted in statistical physics has identified phases and phase transitions in neural network (NN) models, where models within the same phase exhibit similar characteristics but qualitatively differ across phases. A prominent example is the double-descent phenomenon. Recognizing these transitions is essential for building a deeper understanding of model behavior and the underlying mechanics. So far, these phases are typically studied in isolation or in specific applications. In this paper, we show that phase transitions are a widespread phenomenon. However, identifying phase transitions across different methods requires populations that cover different phases. For that reason, we introduce Phase Transition Model Zoos, a structured collection of neural networks trained on diverse datasets and architectures. These model zoos are carefully designed to help researchers systematically identify and study phase transitions in their methods. We demonstrate the relevance of phase transitions across multiple applications, including fine-tuning, transfer learning, out-of-distribution generalization, pruning, ensembling, and weight averaging. The diversity of applications underscores the universal nature of phase transitions and their impact on different tasks. By providing the first structured dataset specifically designed to capture phase transitions in NNs, we offer a valuable tool for the community to systematically evaluate machine learning methods and improve their understanding of phase behavior across a wide range of applications and architectures.",
        "author": [
            "Konstantin Sch√ºrholt", "L√©o Meynent", "Yefan Zhou", "Yaoqing Yang", "Damian Borth"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Model Zoo"
        ],
        "conference": "",
        "url": "https://openreview.net/forum?id=JlkqReTftJ"
    },

    {
        "id": 15,
        "title": "Accelerating Training with Neuron Interaction and Nowcasting Networks",
        "abstract": "Neural network training can be accelerated when a learnable update rule is used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable update rules can be costly and unstable to train and use. Recently, Jang et al. (2023) proposed a simpler approach to accelerate training based on weight nowcaster networks (WNNs). In their approach, Adam is used for most of the optimization steps and periodically, only every few steps, a WNN nowcasts (predicts near future) parameters. We improve WNNs by proposing neuron interaction and nowcasting (NiNo) networks. In contrast to WNNs, NiNo leverages neuron connectivity and graph neural networks to more accurately nowcast parameters. We further show that in some networks, such as Transformers, modeling neuron connectivity accurately is challenging. We address this and other limitations, which allows NiNo to accelerate Adam training by up to 50% in vision and language tasks.",
        "author": [
            "Boris Knyazev", "Abhinav Moudgil", "Guillaume Lajoie", "Eugene Belilovsky", "Simon Lacoste-Julien"
        ],
        "citation": [
            "Diffusion-Based Neural Network Weights Generation",
            "Parameter Prediction for Unseen Deep Architectures"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Acceleration", "Representation Learning", "Weight"
        ],
        "conference": "ICLR",
        "url": "https://arxiv.org/abs/2409.04434"
    },

    {
        "id": 16,
        "title": "DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion",
        "abstract": "Personalized text-to-image generation has gained significant attention for its capability to generate high-fidelity portraits of specific identities conditioned on user-defined prompts. Existing methods typically involve test-time fine-tuning or incorporating an additional pre-trained branch. However, these approaches struggle to simultaneously address efficiency, identity fidelity, and the preservation of the model's original generative capabilities. In this paper, we propose DiffLoRA, an efficient method that leverages the diffusion model as a hypernetwork to predict personalized Low-Rank Adaptation (LoRA) weights based on the reference images. By incorporating these LoRA weights into the off-the-shelf text-to-image model, DiffLoRA enables zero-shot personalization during inference, eliminating the need for post-processing optimization. Moreover, we introduce a novel identity-oriented LoRA weights construction pipeline to facilitate the training process of DiffLoRA. The dataset generated through this pipeline enables DiffLoRA to produce consistently high-quality LoRA weights. Notably, the distinctive properties of the diffusion model enhance the generation of superior weights by employing probabilistic modeling to capture intricate structural patterns and thoroughly explore the weight space. Comprehensive experimental results demonstrate that DiffLoRA outperforms existing personalization approaches across multiple benchmarks, achieving both time efficiency and maintaining identity fidelity throughout the personalization process.",
        "author": [
            "Yujia Wu", "Yiming Shi", "Jiwei Wei", "Chengwei Sun", "Yang Yang", "Heng Tao Shen"
        ],
        "citation": [
            "Conditional LoRA Parameter Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "LoRA", "Diffusion", "Weight Generation", "Personalization"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2408.06740"
    },

    {
        "id": 17,
        "title": "In-Context Meta LoRA Generation",
        "abstract": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task specific fine-tuning. However, in scenarios that involve multiple tasks, training a separate LoRA model for each one results in considerable inefficiency in terms of storage and inference. Moreover, existing parameter generation methods fail to capture the correlations among these tasks, making multi-task LoRA parameter generation challenging. To address these limitations, we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently achieves task-specific customization of large language models (LLMs). Specifically, we use training data from all tasks to train a tailored generator, Conditional Variational Autoencoder (CVAE). CVAE takes task descriptions as inputs and produces task-aware LoRA weights as outputs. These LoRA weights are then merged with LLMs to create task-specialized models without the need for additional fine-tuning. Furthermore, we utilize in-context meta-learning for knowledge enhancement and task mapping, to capture the relationship between tasks and parameter distributions. As a result, our method achieves more accurate LoRA parameter generation for diverse tasks using CVAE. ICM-LoRA enables more accurate LoRA parameter reconstruction than current parameter reconstruction methods and is useful for implementing task-specific enhancements of LoRA parameters. At the same time, our method occupies 283MB, only 1% storage compared with the original LoRA.",
        "author": [
            "Yihua Shao", "Minxi Yan", "Yang Liu", "Siyu Chen", "Wenjie Chen", "Xinwei Long", "Ziyang Yan", "Lei Li", "Chenyu Zhang", "Nicu Sebe", "Hao Tang", "Yan Wang", "Hao Zhao", "Mengzhu Wang", "Jingcai Guo"
        ],
        "citation": [
            "Conditional LoRA Parameter Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "LoRA", "Weight Generation", "In-Context Learining", "Dataset Consendation"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2501.17635"
    },

    {
        "id": 18,
        "title": "Generating GFlowNets as You Wish with Diffusion Process",
        "abstract": "Generative Flow Networks (GFlowNets) are probabilistic samplers that learn stochastic policies to generate diverse sets of high-reward objects, which is essential in scientific discovery tasks. However, most existing GFlowNets necessitate training, becoming costly as the diversity of GFlowNets expands and trajectory lengths increase. To alleviate this problem, we propose a method to Generate high-performing GFlowNet parameters based on a given model structure, called GenFlowNet. Specifically, we first prepare an autoencoder to extract latent representations of GeFlowNet parameters and reconstruct them. Then, a structure encoder is trained alongside a conditional latent diffusion model to generate the target GFlowNet parameters based on the given structure information. To the best of our knowledge, it is the first exploration to generate parameters of a probabilistic sampler using the diffusion process. It enables us to obtain a new GFlowNet without training, effectively reducing the trial-and-error cost during GFlowNet development. Extensive experiments on diverse structures and tasks validate the superiority and generalizability of our method.",
        "author": [
            "Yuxin Li", "Wangbo Zhao", "Dongwen Tang", "jiyao liu", "Xiahai Zhuang", "Guang Li", "Dianbo Liu", "Yang You", "Kai Wang"
        ],
        "citation": [
            "Conditional LoRA Parameter Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Encoding", "Flow", "Diffusion", "Weight Generation"
        ],
        "conference": "",
        "url": "https://openreview.net/forum?id=8ljEGpXuqB"
    },

    {
        "id": 19,
        "title": "Investigating Fine-Tuning of Language Models for Multiple-Choice Questions",
        "abstract": "This thesis investigates the positional and contextual bias of large language models (LLMs) when used to answer multiple-choice questions (MCQs). Given the increasing use of generative language models in fields ranging from cybersecurity to biomedical research, it is important to understand the causes of their behavior in order to mitigate biases and prevent errors. One known method of improving the performance of LLMs is fine-tuning, wherein a model is additionally trained on data from a specified distribution or subject area. We specifically investigate training data properties related to positional bias in fine-tuned language model performance on correctly answering MCQs. To improve model efficiency, we used parameter-efficient fine-tuning, specifically LoRA (Low-Rank Adaptation), which reduces the dimensionality of weight matrices used in the model‚Äôs layers. We verify that if the training data for the model possesses the same qualities and distributions as the test data, the LLM will achieve the best performance. In our experiments, we scaled and balanced our fine-tuning datasets and learned that both processes improve the accuracy on test sets of MCQs.",
        "author": [
            "Wang", "Ivy A."
        ],
        "citation": [
            "Conditional LoRA Parameter Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "LLM", "Fine-Tuning", "Multi-Choice Question"
        ],
        "conference": "",
        "url": "https://dspace.mit.edu/handle/1721.1/157591"
    },

    {
        "id": 20,
        "title": "Make Optimization Once and for All with Fine-grained Guidance",
        "abstract": "Learning to Optimize (L2O) enhances optimization efficiency with integrated neural networks. L2O paradigms achieve great outcomes, e.g., refitting optimizer, generating unseen solutions iteratively or directly. However, conventional L2O methods require intricate design and rely on specific optimization processes, limiting scalability and generalization. Our analyses explore general framework for learning optimization, called Diff-L2O, focusing on augmenting sampled solutions from a wider view rather than local updates in real optimization process only. Meanwhile, we give the related generalization bound, showing that the sample diversity of Diff-L2O brings better performance. This bound can be simply applied to other fields, discussing diversity, mean-variance, and different tasks. Diff-L2O's strong compatibility is empirically verified with only minute-level training, comparing with other hour-levels.",
        "author": [
            "Mingjia Shi", "Ruihan Lin", "Xuxi Chen", "Yuhao Zhou", "Zezhen Ding", "Pingzhi Li", "Tong Wang", "Kai Wang", "Zhangyang Wang", "Jiheng Zhang", "Tianlong Chen"
        ],
        "citation": [
            "Recurrent Diffusion for Large-Scale Parameter Generation"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Optimization", "Diffusion"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2503.11462"
    },

    {
        "id": 21,
        "title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication",
        "abstract": "Large Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLM's trainable parameters the better its ability to implement code. However, when it comes to deploying LLM-based code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. A previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present a differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.",
        "author": [
            "Alessandro Giagnorio", "Antonio Mastropaolo", "Saima Afrin", "Massimiliano Di Penta", "Gabriele Bavota"
        ],
        "citation": [
            "ReALLM: A general framework for LLM compression and fine-tuning"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Quantization", "LLM", "Code Geneeration"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2503.07103"
    },

    {
        "id": 22,
        "title": "RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy",
        "abstract": "Low-rank adaptation (LoRA) has become the dominant method for parameter-efficient LLM fine-tuning, with LoRA-based quantization error compensation (LQEC) emerging as a powerful tool for recovering accuracy in compressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with no prior investigation into understanding this limitation. We propose RILQ (Rank-Insensitive LoRA-based Quantization Error Compensation) to understand fundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis revealing model-wise activation discrepancy loss's rank-insensitive nature, RILQ employs this loss to adjust adapters cooperatively across layers, enabling robust error compensation with low-rank adapters. Evaluations on LLaMA-2 and LLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inference across various state-of-the-art quantizers and enhanced accuracy in task-specific fine-tuning. RILQ maintains computational efficiency comparable to existing LoRA methods, enabling adapter-merged weight-quantized LLM inference with significantly enhanced accuracy, making it a promising approach for boosting 2-bit LLM performance. Our code is available at this https URL.",
        "author": [
            "Geonho Lee", "Janghwan Lee", "Sukjin Hong", "Minsoo Kim", "Euijai Ahn", "Du-Seong Chang", "Jungwook Choi"
        ],
        "citation": [
            "ReALLM: A general framework for LLM compression and fine-tuning"
        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "LoRA", "LLM", "Quantization"
        ],
        "conference": "",
        "url": "https://arxiv.org/abs/2412.01129"
    },

    {
        "id": 23,
        "title": "Graph HyperNetworks for Neural Architecture Search",
        "abstract": "Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast -- they can search nearly 10 times faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs.",
        "author": [
            "Chris Zhang", "Mengye Ren", "Raquel Urtasun"
        ],
        "citation": [

        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Neural Architecture Search", "Graph Hyper Network"
        ],
        "conference": "ICLR",
        "url": "https://arxiv.org/abs/1810.05749"
    },

    {
        "id": 24,
        "title": "Parameter Prediction for Unseen Deep Architectures",
        "abstract": "Deep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefficient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowledge of training other networks. We introduce a large-scale dataset of diverse computational graphs of neural architectures - DeepNets-1M - and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis.",
        "author": [
            "Boris Knyazev", "Michal Drozdzal", "Graham W. Taylor", "Adriana Romero-Soriano"
        ],
        "citation": [

        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Prediction", "Weight"
        ],
        "conference": "NeurIPS",
        "url": "https://arxiv.org/abs/2110.13100"
    },

    {
        "id": 25,
        "title": "Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?",
        "abstract": "Pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. We aim at an ambitious goal of democratizing pretraining. Towards that goal, we train and release a single neural network that can predict high quality ImageNet parameters of other neural networks. By using predicted parameters for initialization we are able to boost training of diverse ImageNet models available in PyTorch. When transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.",
        "author": [
            "Boris Knyazev", "Doha Hwang", "Simon Lacoste-Julien"
        ],
        "citation": [

        ],
        "Area": "Engineering & Computing",
        "keyword": [
            "Prediction", "Graph Hyper Network", "Weight"
        ],
        "conference": "ICML",
        "url": "https://arxiv.org/abs/2303.04143"
    }
]